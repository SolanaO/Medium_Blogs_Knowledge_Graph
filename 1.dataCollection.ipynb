{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5a62db",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438248f",
   "metadata": {},
   "source": [
    "## Workspace Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general libraries\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "# datasets and numerical manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# libraries for scraping and selecting information\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# create a set of options for the webdriver\n",
    "options = Options()\n",
    "options.add_argument(' — no-sandbox')\n",
    "options.add_argument(' — disable-gpu')\n",
    "options.add_argument(' — incognito')\n",
    "options.add_argument(' — disable-dev-shm-usage')\n",
    "options.add_argument(' — ignore-certificate-errors')\n",
    "options.add_argument(' — ignore-ssl-errors')\n",
    "options.add_argument(' — headless')\n",
    "#options.add_argument(' — start-maximized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b8faf",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8333d3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTES:</b>\n",
    "\n",
    "- In order to access the articles published during a certain period of time, we need to know how the date is represented in the URL of the page. For example, if we want all the articles published by Towards Data Science on February 22, 2022 we type: \n",
    "    - <em>towardsdatascience.com/archive/2022/02/22</em>\n",
    "\n",
    "- We create functions that specify the month and the day, to pass to the webdriver.\n",
    "\n",
    "- The first publications in Towards Data Science date from 2010, the first year when the publications are grouped by months is 2015. In the first couple of years there are months with no published articles.\n",
    "    \n",
    "   </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many_days(year, month):\n",
    "    \n",
    "    '''\n",
    "    Function to get the number of days in a month.\n",
    "    It does take into account if the year is bisect or not.\n",
    "    \n",
    "    INPUT:\n",
    "        year (int) - the year\n",
    "        month (int) - an integer between 1 and 12 for the month\n",
    "    OUTPUT:\n",
    "        n_days (int) - number of days in the month\n",
    "    '''\n",
    "    if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        n_days = 31\n",
    "    elif month in [4, 6, 9, 11]:\n",
    "        n_days = 30\n",
    "    elif ((month == 2) & (year % 4 != 0)): \n",
    "        n_days = 28 \n",
    "    elif ((month == 2) & (year % 4 == 0)): \n",
    "        n_days = 29\n",
    "    return n_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfdfde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_date_strings(year, month, day):\n",
    "    \n",
    "    '''\n",
    "    Function to print the date as a string in format needed\n",
    "    to complete the url for the web request.\n",
    "    \n",
    "    If day is 0 the string contains the year and the month only.\n",
    "    If both day and month are 0, the string contains the year only.\n",
    "    \n",
    "    INPUT:\n",
    "        year (int) - represents the year\n",
    "        month (int) - the month as an integer from 0 to 1\n",
    "        day (int) - the day of the month\n",
    "        \n",
    "    OUTPUT:\n",
    "       date (str) in format YYYY/MM/DD, YYYY/MM or YYYY\n",
    "    '''\n",
    "    \n",
    "    if ((month == 0) and (day == 0)):\n",
    "        date_year = f'{year}'\n",
    "        return date_year\n",
    "    \n",
    "    elif (day == 0):\n",
    "        \n",
    "        month = str(month)\n",
    "        \n",
    "        if len(month) == 1:\n",
    "            month = f'0{month}'\n",
    "        \n",
    "        date_month = f'{year}/{month}'\n",
    "        return date_month\n",
    "    \n",
    "    else:\n",
    "        month, day = str(month), str(day)\n",
    "        \n",
    "        if len(month) == 1:\n",
    "            month = f'0{month}'\n",
    "            \n",
    "        if len(day) == 1:\n",
    "            day = f'0{day}'\n",
    "        \n",
    "        date_full = f'{year}/{month}/{day}'\n",
    "        return date_full\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d987f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_retrieve(publication, year, months, days):\n",
    "    \n",
    "    '''\n",
    "    Creates a list of urls to use for retrieving data from the website.\n",
    "    If the whole year is desired enter [0] for both months and days.\n",
    "    If full month(s) are desired, enter [0] for days.\n",
    "    \n",
    "    INPUT:\n",
    "        publication (str) - publication name to be included in url\n",
    "        year (int) - the desired year \n",
    "        months (list) - list of months to retrieve from\n",
    "        days (list) - list of days to retrieve from\n",
    "    OUTPUT:\n",
    "        urls (list) - urls for the specified days, months or years\n",
    "    '''\n",
    "    \n",
    "    list_urls = []\n",
    "    root_url = f'https://{publication}.com/archive/'\n",
    "    \n",
    "    for month in months:\n",
    "        for day in days:\n",
    "            formated_date = url_date_strings(year,month,day)\n",
    "            page_url = root_url + formated_date\n",
    "            list_urls.append(page_url)\n",
    "    \n",
    "    return list_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3db53d",
   "metadata": {},
   "source": [
    "## Functions to Collect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151fc582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(publication, year, months, days):\n",
    "    '''\n",
    "    The function performs the following steps:\n",
    "        - scrape the data from the provided urls\n",
    "        - creates a BeautifulSoup object\n",
    "        - extracts information about each post \n",
    "        - stores all the extracted information in a list\n",
    "        \n",
    "    INPUT:\n",
    "        publication (str) - name of publication as it appears on url\n",
    "        year (int) - a year between 2010 and 2022 \n",
    "        months(list) - a list of months to scrape data from\n",
    "        days (list) - days to scrape data from\n",
    "    OUTPUT:\n",
    "        stories_info (list) - list that contain extracted information\n",
    "                              for each post on the specified dates\n",
    "     '''\n",
    "    \n",
    "    # instantiate the driver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # empty list to collect the information for each post   \n",
    "    data_stories = []\n",
    "    \n",
    "    # list of urls for the specified dates\n",
    "    list_urls = url_to_retrieve(publication, year, months, days)\n",
    "    \n",
    "    for url in list_urls:\n",
    "        # send the request to retrieve the information\n",
    "        source = driver.get(url)\n",
    "            \n",
    "        # create a string object for the page source\n",
    "        source_string = driver.page_source\n",
    "            \n",
    "        # create a beautiful soup object\n",
    "        soup = BeautifulSoup(source_string,'lxml')\n",
    "            \n",
    "        # find all the stories published during the specified period\n",
    "        stories = soup.find_all('div',\n",
    "                                class_='streamItem streamItem--postPreview js-streamItem')\n",
    "            \n",
    "        for story in stories:\n",
    "            author_box = story.find('div',\n",
    "                                    class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')\n",
    "            author_url = author_box.find('a')['href']\n",
    "            user_id = author_box.find('a')['data-user-id']\n",
    "            date = author_box.find('time')['datetime']\n",
    "            reading_time = author_box.find('span', class_='readingTime')['title']\n",
    "            \n",
    "            title = story.find('h3').text if story.find('h3') else '-'\n",
    "            subtitle = story.find('h4').text if story.find('h4') else '-'\n",
    "        \n",
    "            if story.find('button',\n",
    "                          class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents'):\n",
    "                claps = story.find('button',\n",
    "                                   class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents').text\n",
    "            else:\n",
    "                claps = 0\n",
    "                    \n",
    "            if story.find('a', class_='button button--chromeless u-baseColor--buttonNormal'):\n",
    "                responses = story.find('a',\n",
    "                                       class_='button button--chromeless u-baseColor--buttonNormal').text\n",
    "            else:\n",
    "                responses = '0 responses'\n",
    "    \n",
    "\n",
    "            story_url = story.find('a', \n",
    "                                   class_='button button--smaller button--chromeless u-baseColor--buttonNormal')['href']\n",
    "    \n",
    "            \n",
    "            # send the request to retrieve the story information\n",
    "            source_story = driver.get(story_url)\n",
    "            # create a string object for the story's page source\n",
    "            source_string_story = driver.page_source\n",
    "            # create a beautiful soup object \n",
    "            soup_story = BeautifulSoup(source_string_story,'lxml')\n",
    "            \n",
    "            # retrieve the full subtitle, place a marker if no subtitle is found\n",
    "            if soup_story.find('h2', class_='pw-subtitle-paragraph'):\n",
    "                full_subtitle = soup_story.find('h2', class_='pw-subtitle-paragraph').text\n",
    "            else:\n",
    "                full_subtitle = '-'\n",
    "        \n",
    "            # collect all the information and add it to the list\n",
    "            data_stories.append([author_url, user_id, date, reading_time, \n",
    "                             title, subtitle, full_subtitle, claps, responses, story_url])\n",
    "        # take a break\n",
    "        sleep(np.random.randint(1, 10))\n",
    "    \n",
    "    return data_stories    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c96c34f",
   "metadata": {},
   "source": [
    "## Collect and Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f41d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "publication = 'towardsdatascience'\n",
    "data_cols = ['author_url', 'user_id', 'date', 'reading_time', \n",
    "         'title', 'subtitle', 'claps', 'responses', 'story_url'] \n",
    "\n",
    "def retrieve_publications(publication, year):\n",
    "    \n",
    "    '''\n",
    "    The function cycles through the days and months of one year to extract \n",
    "    the information. The data is saved in a csv file.\n",
    "    \n",
    "    INPUT:\n",
    "        publication (str) - name of publication as it appears on url\n",
    "        year (int) - a year between 2010 and 2022 \n",
    "    OUTPUT:\n",
    "        none - data is saved in a file\n",
    "    '''\n",
    "    \n",
    "    for i in range(1, 13):\n",
    "        #the number of days in the particular month\n",
    "        num_days = how_many_days(year, i)\n",
    "        # scrape all the blogs from that year\n",
    "        all_month = retrieve_metadata(publication, year, [i], range(1, num_days+1))\n",
    "        # create a dataframe from the retrieved data\n",
    "        month_df = pd.DataFrame(all_month, columns=data_cols)\n",
    "        # save the dataframe into a file\n",
    "        month_df.to_csv(f'data/tds_{year}_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_publications(publication, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41c73b",
   "metadata": {},
   "source": [
    "## Collect data in stages - Alternative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfd56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreve metadata from daily archive pages\n",
    "def retrieve_metadata(publication, year, months, days):\n",
    "    '''\n",
    "    The function performs the following steps:\n",
    "        - scrape the data from the provided urls\n",
    "        - creates a BeautifulSoup object\n",
    "        - extracts information about each post \n",
    "        - stores all the extracted information in a list\n",
    "        \n",
    "    INPUT:\n",
    "        publication (str) - name of publication as it appears on url\n",
    "        year (int) - a year between 2010 and 2022 \n",
    "        months(list) - a list of months to scrape data from\n",
    "        days (list) - days to scrape data from\n",
    "    OUTPUT:\n",
    "        stories_info (list) - list of lists that contain extracted information\n",
    "                              for each post on the specified dates\n",
    "     '''\n",
    "    \n",
    "    # instantiate the driver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # empty list to collect the posts information    \n",
    "    data_stories = []\n",
    "    \n",
    "    # list of urls for the specified dates\n",
    "    list_urls = url_to_retrieve(publication, year, months, days)\n",
    "    \n",
    "    for url in list_urls:\n",
    "        # send the request to retrieve the information\n",
    "        source = driver.get(url)\n",
    "            \n",
    "        # create a string object for the page source\n",
    "        source_string = driver.page_source\n",
    "            \n",
    "        # create a beautiful soup object\n",
    "        soup = BeautifulSoup(source_string,'lxml')\n",
    "            \n",
    "        # find all the stories published during the specified period\n",
    "        stories = soup.find_all('div',\n",
    "                                class_='streamItem streamItem--postPreview js-streamItem')\n",
    "            \n",
    "        for story in stories:\n",
    "            author_box = story.find('div',\n",
    "                                    class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')\n",
    "            author_url = author_box.find('a')['href']\n",
    "            user_id = author_box.find('a')['data-user-id']\n",
    "            date = author_box.find('time')['datetime']\n",
    "            reading_time = author_box.find('span', class_='readingTime')['title']\n",
    "            \n",
    "            # take a break\n",
    "            sleep(np.random.randint(1, 6))\n",
    "            \n",
    "            title = story.find('h3').text if story.find('h3') else '-'\n",
    "            # some subtitles are truncated in the archive page\n",
    "            trunc_subtitle = story.find('h4').text if story.find('h4') else '-'\n",
    "        \n",
    "            if story.find('button',\n",
    "                          class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents'):\n",
    "                claps = story.find('button',\n",
    "                                   class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents').text\n",
    "            else:\n",
    "                claps = 0\n",
    "                    \n",
    "            if story.find('a', class_='button button--chromeless u-baseColor--buttonNormal'):\n",
    "                responses = story.find('a',\n",
    "                                       class_='button button--chromeless u-baseColor--buttonNormal').text\n",
    "            else:\n",
    "                responses = '0 responses'\n",
    "    \n",
    "\n",
    "            story_url = story.find('a', \n",
    "                                   class_='button button--smaller button--chromeless u-baseColor--buttonNormal')['href']\n",
    "        \n",
    "            # collect all the information and add it to the list\n",
    "            data_stories.append([author_url, user_id, date, reading_time, \n",
    "                             title, trunc_subtitle, claps, responses, story_url])\n",
    "    return data_stories    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_subtitle(stories_list, filename):\n",
    "    '''\n",
    "    The function performs the following steps:\n",
    "        - scrape the data from the provided urls\n",
    "        - creates a BeautifulSoup object\n",
    "        - extracts title and subtitle for each post \n",
    "        - stores all the extracted information in a list\n",
    "        - saves the data in a csv file\n",
    "        \n",
    "    INPUT:\n",
    "        stories_list (list) - list of individual post urls\n",
    "        filename (str) - name of file where the data is saved\n",
    "    OUTPUT:\n",
    "        none - data is saved in a file\n",
    "     '''\n",
    "    \n",
    "    # instantiate the driver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # empty list to collect the posts information    \n",
    "    data_stories = []\n",
    "    \n",
    "    for url in stories_list:\n",
    "        \n",
    "        # send the request to retrieve the information\n",
    "        source = driver.get(url)\n",
    "            \n",
    "        # create a string object for the page source\n",
    "        source_string = driver.page_source\n",
    "            \n",
    "        # create a beautiful soup object\n",
    "        soup = BeautifulSoup(source_string,'lxml')\n",
    "            \n",
    "        title = soup.find('h1').text\n",
    "        \n",
    "        if soup.find('h2', class_='pw-subtitle-paragraph'):\n",
    "            subtitle = soup.find('h2', class_='pw-subtitle-paragraph').text\n",
    "        elif soup.find('h3', class_='pw-subtitle-paragraph'):\n",
    "            subtitle = soup.find('h3', class_='pw-subtitle-paragraph').text\n",
    "        else:\n",
    "            subtitle = '-'\n",
    "        \n",
    "        # collect all the information and add it to the list\n",
    "        data_stories.append([url, title, subtitle])\n",
    "        \n",
    "    # create a dataframe from the retrieved data\n",
    "    subtitles_df = pd.DataFrame(data_stories, columns=['url', 'title', 'subtitle'])\n",
    "    # save the dataframe into a file\n",
    "    subtitles_df.to_csv(filename, index=False) \n",
    "    \n",
    "    # take a break\n",
    "    sleep(np.random.randint(1, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_all_subtitles(year, months):\n",
    "    \n",
    "    '''\n",
    "    The function cycles through the files to extract article urls,\n",
    "    after which it scrapes the title and the full subtitle for each post.\n",
    "    The information is saved to a file.\n",
    "    \n",
    "    INPUT:\n",
    "        year (int) - a year between 2010 and 2022 \n",
    "        months (list) - specified months as a list\n",
    "    OUTPUT:\n",
    "        none - data is saved to a file\n",
    "    '''\n",
    "    \n",
    "    for i in months:\n",
    "        \n",
    "        # create a dataframe that contains the list of urls\n",
    "        stories_df = pd.read_csv(f'tds_{year}_{i}.csv')\n",
    "        \n",
    "        # create a list of urls\n",
    "        stories_urls = stories_df.story_url\n",
    "        # provide the names of the files\n",
    "        filename = f'sub_tds_{year}_{i}.csv'\n",
    "        # extract the subtitles and save the data to a file\n",
    "        retrieve_subtitle(stories_urls, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9217bb",
   "metadata": {},
   "source": [
    "## Combine Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file that combines all the individual downloads\n",
    "merged_dfs = pd.concat([pd.read_csv(csv_file, index_col=None) for csv_file in glob.glob(os.path.join('data/data_raw/data_monthly', '*.csv'))],\n",
    "                      axis=0, ignore_index=True)\n",
    "    \n",
    "merged_dfs.to_csv('data/data_raw/merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ccd1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv('data/data_raw/merged.csv', index_col=None)\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0745f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
